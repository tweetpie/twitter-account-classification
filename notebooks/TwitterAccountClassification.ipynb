{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TwitterAccountClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkoPrbyUMqpB"
      },
      "source": [
        "## Import related libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBN6jxZE5gzx"
      },
      "source": [
        "# ALL IMPORTS \n",
        "# -*- coding: utf-8 -*-\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import nltk\n",
        "\n",
        "import numpy as np\n",
        "# # Keras\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# NLTK\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Other\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "# Import data and preprocess\n",
        "import pandas as pd\n",
        "import os\n",
        "from os import listdir\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import inflect as inflect\n",
        "import pandas_profiling\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import KFold\n",
        "from heapq import heappush, heappushpop\n",
        "from sklearn.metrics import *\n",
        "\n",
        "nltk.download('stopwords')\n",
        "pd.set_option('display.max_columns', None)\n",
        "tf.config.experimental_run_functions_eagerly(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eaMaETpMqpO"
      },
      "source": [
        "## Download Sentencepiece models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVISo--TMqpQ"
      },
      "source": [
        "os.system('wget -P ../sentencepiece/ https://nlp.h-its.org/bpemb/multi/multi.wiki.bpe.vs1000000.d300.w2v.bin.tar.gz')\n",
        "os.system('wget -P ../sentencepiece/ https://nlp.h-its.org/bpemb/multi/multi.wiki.bpe.vs1000000.model')\n",
        "os.system('wget -P ../sentencepiece/ https://nlp.h-its.org/bpemb/multi/multi.wiki.bpe.vs1000000.vocab')\n",
        "os.system('tar -xzvf ../sentencepiece/multi.wiki.bpe.vs1000000.d300.w2v.bin.tar.gz -C ../sentencepiece/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp3k2SXaMqpZ"
      },
      "source": [
        "## Import Sentecepiece model and generate embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxQ72TQLVK1x"
      },
      "source": [
        "import sentencepiece as spm\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# IMPORT EMBEDDING DICT\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"../sentencepiece/multi.wiki.bpe.vs1000000.model\")\n",
        "\n",
        "print('Indexing word vectors.')\n",
        "EMBEDDING_DIM = 300\n",
        "max_len = 64\n",
        "\n",
        "word2vec_file = \"../sentencepiece/multi.wiki.bpe.vs1000000.d300.w2v.bin\"\n",
        "vecs = KeyedVectors.load_word2vec_format(word2vec_file, binary=True)\n",
        "\n",
        "embedding_matrix = np.zeros((len(vecs.vocab)+1, EMBEDDING_DIM))\n",
        "\n",
        "for vocab in vecs.vocab:\n",
        "    embedding_matrix[sp.PieceToId(vocab)] = vecs[vocab]\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f4GWwnvMqpk"
      },
      "source": [
        "## Define preprocessing methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoB7fcf5dhmk"
      },
      "source": [
        "# CLEAN TEXT METHOD\n",
        "def replace_numbers(text):\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
        "    p = inflect.engine()\n",
        "    words = text.split()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word.isdigit():\n",
        "            new_word = p.number_to_words(word)\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return ' '.join(new_words)\n",
        "    \n",
        "def clean_text(text):\n",
        "    # Convert I and İ into lowercase\n",
        "    lower_map = {\n",
        "    ord(u'I'): u'ı',\n",
        "    ord(u'İ'): u'i',\n",
        "    }\n",
        "\n",
        "    text = text.translate(lower_map)\n",
        "    \n",
        "    ## Remove puncuation\n",
        "    text = text.translate(string.punctuation)\n",
        "\n",
        "    ## Convert words to lower case and split them\n",
        "    text = text.lower().split()\n",
        "\n",
        "    ## Remove stop words\n",
        "    stops = set(stopwords.words(\"english\")).union(stopwords.words(\"turkish\"))\n",
        "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
        "    text = \" \".join(text)\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub(r'\\n', ' ', text) \n",
        "    return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Gjw3dyRMqpr"
      },
      "source": [
        "## Define Statistical Analysis Method for dataset features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euvzCFq3eYzc"
      },
      "source": [
        "# CALCULATE STATS ABOUT DATASET\n",
        "def calc_seq_stats(data):\n",
        "    avg_bio, avg_name, avg_username, avg_location, avg_url = 0, 0, 0, 0, 0\n",
        "    max_bio, max_name, max_username, max_location, max_url = 0, 0, 0, 0, 0\n",
        "    bio_count, name_count, username_count, location_count, url_count = [], [], [], [], []\n",
        "    for bio in data['bio']:\n",
        "        bio_count.append(len(bio.split()))\n",
        "        max_bio = len(bio.split()) if len(bio.split()) > max_bio else max_bio\n",
        "        avg_bio += len(bio.split())\n",
        "    for name in data['name']:\n",
        "        name_count.append(len(name.split()))\n",
        "        max_name = len(name.split()) if len(name.split()) > max_name else max_name\n",
        "        avg_name += len(name.split())\n",
        "    for username in data['username']:\n",
        "        username_count.append(len(username.split()))\n",
        "        max_username = len(username.split()) if len(username.split()) > max_username else max_username\n",
        "        avg_username += len(username.split())\n",
        "    for location in data['location']:\n",
        "        location_count.append(len(location.split()))\n",
        "        max_location = len(location.split()) if len(location.split()) > max_location else max_location\n",
        "        avg_location += len(location.split())\n",
        "    for url in data['url']:\n",
        "        url_count.append(len(url.split()))\n",
        "        max_url = len(url.split()) if len(url.split()) > max_url else max_url\n",
        "        avg_url += len(url.split())\n",
        "\n",
        "    avg_bio /= len(data['bio'])\n",
        "    avg_name /= len(data['name'])\n",
        "    avg_username /= len(data['username'])\n",
        "    avg_location /= len(data['location'])\n",
        "    avg_url /= len(data['url'])\n",
        "\n",
        "    print('Bio -> mean: {}, std: {}\\n'  \\\n",
        "          'Name -> mean: {}, std: {}\\n' \\\n",
        "          'Username -> mean: {}, std: {}\\n' \\\n",
        "          'Location -> mean: {}, std: {}\\n' \\\n",
        "          'Url -> mean: {}, std: {}\\n'.format(\n",
        "              np.mean(np.asarray(bio_count)), np.std(np.asarray(bio_count)),\n",
        "              np.mean(np.asarray(name_count)), np.std(np.asarray(name_count)),\n",
        "              np.mean(np.asarray(username_count)), np.std(np.asarray(username_count)),\n",
        "              np.mean(np.asarray(location_count)), np.std(np.asarray(location_count)),\n",
        "              np.mean(np.asarray(url_count)), np.std(np.asarray(url_count))\n",
        "          ))\n",
        "\n",
        "    print('Average Sequence Lengths:\\nBio: {}\\nName: {}\\nUsername: {}\\nLocation: {}\\nUrl: {}\\n'.format(avg_bio, avg_name, avg_username, avg_location, avg_url))\n",
        "    print('Max Sequence Lengths:\\nBio: {}\\nName: {}\\nUsername: {}\\nLocation: {}\\nUrl: {}\\n'.format(max_bio, max_name, max_username, max_location, max_url))\n",
        "\n",
        "    print('---------------Numeric Features Analysis------------')\n",
        "    pd.set_option('display.max_colwidth', -1)\n",
        "    pd.set_option('display.notebook_repr_html', True)\n",
        "    pd.set_option('precision', 2)\n",
        "    print('---------Max values-----------')\n",
        "    print(data[['tweets', 'following', 'followers', 'likes', 'media']].max())\n",
        "    print('---------Min values-----------')\n",
        "    print(data[['tweets', 'following', 'followers', 'likes', 'media']].min())\n",
        "    print('---------Average values-----------')\n",
        "    print(data[['tweets', 'following', 'followers', 'likes', 'media']].mean())\n",
        "    print('---------Standard Deviation values-----------')\n",
        "    print(data[['tweets', 'following', 'followers', 'likes', 'media']].std(ddof=1))\n",
        "    print('-----------------------------------------------------------')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlYYhcRgMqpx"
      },
      "source": [
        "## Import Data and Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhkQhXGwdzBE"
      },
      "source": [
        "\n",
        "# IMPORT DATA INTO PANDAS DATAFRAME \n",
        "dataset_path = '../dataset/dataset.csv'\n",
        "\n",
        "data = pd.read_csv(dataset_path)\n",
        "print('Data shape: ', data.shape)\n",
        "\n",
        "# # Discard not labeled data\n",
        "data = data.loc[(data['label'] == '0') | (data['label'] == '1')]\n",
        "# Replace null values with empty string\n",
        "data = data.replace(np.nan, '', regex=True)\n",
        "# Make sure to truncate rows with corrupted labels.\n",
        "\n",
        "print(len(alo))\n",
        "data = data.loc[(data['label'] == '0') | (data['label'] == '1')]\n",
        "\n",
        "# Make sure that duplicate users eliminated\n",
        "#################################################\n",
        "# looking at stats\n",
        "# usefull to inspect and explain dataset\n",
        "# pandas_profiling.ProfileReport(data)\n",
        "#################################################\n",
        "print(data['label'].value_counts())\n",
        "\n",
        "# Eliminate duplicate rows using username column\n",
        "usernames = data[\"username\"]\n",
        "duplicate_usernames = pd.concat(g for _, g in data.groupby('username') if len(g) > 1)\n",
        "duplicate_usernames = duplicate_usernames.drop_duplicates(subset='username', keep='first')\n",
        "\n",
        "data = data.drop_duplicates(subset='username', keep='first')\n",
        "print('------After Duplicate Elimination------')\n",
        "print('Duplicated account number: ', duplicate_usernames.shape[0])\n",
        "print(':::Data Values:::')\n",
        "print('Data shape: ', data.shape)\n",
        "print(data['label'].value_counts())\n",
        "orig_data = data.copy()\n",
        "print(orig_data.shape)\n",
        "\n",
        "# Clean Text Features\n",
        "data['bio'] = data['bio'].map(lambda x: replace_numbers(clean_text(x)))\n",
        "data['name'] = data['name'].map(lambda x: replace_numbers(clean_text(x)))\n",
        "# data['username'] = data['username'].map(lambda x: clean_text(x))\n",
        "data['location'] = data['location'].map(lambda x: replace_numbers(clean_text(x)))\n",
        "data['url'] = data['url'].map(lambda x: replace_numbers(clean_text(x.split('com')[0])))\n",
        "\n",
        "# Interpolate(Scale) Number Features\n",
        "scaler = MinMaxScaler()\n",
        "data[['tweets']] = scaler.fit_transform(data[['tweets']])\n",
        "data[['following']] = scaler.fit_transform(data[['following']])\n",
        "data[['followers']] = scaler.fit_transform(data[['followers']])\n",
        "data[['likes']] = scaler.fit_transform(data[['likes']])\n",
        "data[['media']] = scaler.fit_transform(data[['media']])\n",
        "\n",
        "data['label'] = data['label'].astype('int32')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJkTJ5SFeUcK"
      },
      "source": [
        "# PRINT STATS\n",
        "calc_seq_stats(orig_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jZNaJRKMqp6"
      },
      "source": [
        "## Define postprocess method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVx1BmHaent5"
      },
      "source": [
        "def postProcessDataset(input_data):\n",
        "    combined_text_features = np.asarray(['{} {} {} {}'.format(bio,name,location,url) for bio,name,location,url in zip(input_data['bio'], input_data['name'], input_data['location'], input_data['url'])]) \n",
        "    text_data_vec = tf.keras.preprocessing.sequence.pad_sequences([sp.EncodeAsIds(x) for x in combined_text_features], maxlen=max_len)\n",
        "    number_data_vec = np.asarray([[x, y, z, t, w] for x, y, z, t, w in zip(input_data['tweets'], input_data['following'], input_data['followers'], input_data['likes'], input_data['media'])])\n",
        "    return text_data_vec, number_data_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6cAS5Q0Mqp9"
      },
      "source": [
        "## Define shortcut variables for Tensorflow units"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYDQrYB9zCYA"
      },
      "source": [
        "# TENSORFLOW CODES\n",
        "Sequential = tf.keras.models.Sequential\n",
        "Dense = tf.keras.layers.Dense\n",
        "Dropout = tf.keras.layers.Dropout\n",
        "Flatten = tf.keras.layers.Flatten\n",
        "Conv2D = tf.keras.layers.Conv2D\n",
        "Embedding = tf.keras.layers.Embedding\n",
        "Bidirectional = tf.keras.layers.Bidirectional\n",
        "LSTM = tf.keras.layers.LSTM\n",
        "Model = tf.keras.Model\n",
        "Sequential = tf.keras.Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SsUHjCo0F8m"
      },
      "source": [
        "# MULTILAYER PERCEPTRON\n",
        "def create_mlp(dim):\n",
        "    # define our MLP network\n",
        "    model = Sequential()\n",
        "    model.add(Dense(16, input_dim=dim, activation=\"relu\"))\n",
        "    model.add(Dropout(rate=0.5))\n",
        "    model.add(Dense(32, activation=\"relu\"))\n",
        "\n",
        "\n",
        "    # return our model\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMzckaFnMqqC"
      },
      "source": [
        "## Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_4zdpmu0Iem"
      },
      "source": [
        "#BUILD MODEL\n",
        "class MyModel(Model):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.embedding1 = Embedding(len(vecs.vocab)+1, EMBEDDING_DIM, input_length=max_len,\n",
        "                                    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "                                    trainable=False)\n",
        "        self.lstm1 = Bidirectional(LSTM(64))\n",
        "        self.dropout = Dropout(rate=0.5)\n",
        "        self.mlp = create_mlp(10)\n",
        "        self.d1 = Dense(16, activation='relu')\n",
        "        self.d3 = Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, x, y):\n",
        "        x = self.embedding1(x)\n",
        "        x = self.lstm1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.d1(x)\n",
        "\n",
        "        y = self.mlp(y)\n",
        "\n",
        "        z = tf.concat([x, y], -1)\n",
        "\n",
        "        return self.d3(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5KXQWMZMqqF"
      },
      "source": [
        "## Define train and test step methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Wr_4tic0z0a"
      },
      "source": [
        "@tf.function\n",
        "def train_step(data1, data2, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(data1, data2)\n",
        "        loss = loss_object(labels, predictions)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(grads_and_vars=list(zip(gradients, model.trainable_variables)))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, predictions)\n",
        "\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=optimizer.iterations)\n",
        "    tf.summary.scalar('accuracy', train_accuracy.result(), step=optimizer.iterations)\n",
        "    \n",
        "@tf.function\n",
        "def test_step(data1, data2, labels, step_num):\n",
        "    predictions = model(data1, data2)\n",
        "    t_loss = loss_object(labels, predictions)\n",
        "\n",
        "    test_loss(t_loss)\n",
        "    test_accuracy(labels, predictions)\n",
        "\n",
        "    tf.summary.scalar('loss', test_loss.result(), step=step_num)\n",
        "    tf.summary.scalar('accuracy', test_accuracy.result(), step=step_num)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu_7VGvDMqqI"
      },
      "source": [
        "## Train model with k-fold where k=7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpl8fNSV02D5"
      },
      "source": [
        "model_name = 'adaptive_adam_128_batch_100_epoch_equal_feature_weights'\n",
        "kf = KFold(n_splits=7, random_state=None, shuffle=True)\n",
        "k_count = 0\n",
        "for train_index, test_index in kf.split(range(len(data.index))):\n",
        "    # CREATE AN INSTANCE OF THE MODEL\n",
        "    model = MyModel()\n",
        "\n",
        "    # Setting learning rate with learning rate decay function\n",
        "    global_step = tf.Variable(0, trainable=False)\n",
        "    starter_learning_rate = 0.001\n",
        "    end_learning_rate = 0.0001\n",
        "    decay_steps = 25\n",
        "    learning_rate = tf.compat.v1.train.polynomial_decay(starter_learning_rate, global_step, decay_steps, end_learning_rate, power=0.5)\n",
        "\n",
        "    loss_object = tf.keras.losses.BinaryCrossentropy()\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "    train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
        "\n",
        "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "    test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')\n",
        "    # -------------------------------------------------------------------\n",
        "    best_accuracies, best_accuracies_epoch = [0,0,0,0,0], [0,0,0,0,0]\n",
        "    k_count += 1\n",
        "    train_data = data.iloc[list(train_index), :] \n",
        "    test_data = data.iloc[list(test_index), :] \n",
        "\n",
        "    tf.io.gfile.makedirs('../models/model_{}/k_{}'.format(model_name, k_count))\n",
        "\n",
        "    train_data, validation_data = np.split(train_data.sample(frac=1), [int(.8*len(train_data))])\n",
        "\n",
        "    train_labels = tf.constant(train_data['label'].to_numpy().reshape(train_data['label'].shape[0], 1))\n",
        "    test_labels = tf.constant(test_data['label'].to_numpy().reshape(test_data['label'].shape[0], 1))\n",
        "    validation_labels = tf.constant(validation_data['label'].to_numpy().reshape(validation_data['label'].shape[0], 1))\n",
        "\n",
        "    train_text_data_vec, train_number_data_vec = postProcessDataset(train_data)\n",
        "    validation_text_data_vec, validation_number_data_vec = postProcessDataset(validation_data)\n",
        "    test_text_data_vec, test_number_data_vec = postProcessDataset(test_data)\n",
        "\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((train_text_data_vec, train_number_data_vec, train_labels)).shuffle(len(train_labels)).batch(128)\n",
        "    validation_ds = tf.data.Dataset.from_tensor_slices((validation_text_data_vec, validation_number_data_vec, validation_labels)).shuffle(len(validation_labels)).batch(128)\n",
        "    test_ds = tf.data.Dataset.from_tensor_slices((test_text_data_vec, test_number_data_vec, test_labels)).batch(1)\n",
        "\n",
        "    local_logdir = '../summaries/model_{}/k_{}'.format(model_name, k_count)\n",
        "    train_summary_writer = tf.summary.create_file_writer('{}/train'.format(local_logdir))\n",
        "    test_summary_writer = tf.summary.create_file_writer('{}/validation'.format(local_logdir))\n",
        "\n",
        "    EPOCHS = 101\n",
        "    for epoch in range(EPOCHS):\n",
        "        for text, number, labels in train_ds:\n",
        "            with train_summary_writer.as_default():\n",
        "                train_step(text, number,labels)\n",
        "        for text, number, labels in validation_ds:\n",
        "            with test_summary_writer.as_default():    \n",
        "                test_step(text, number, labels, optimizer.iterations)\n",
        "\n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}, validation Loss: {}, validation Accuracy: {}'\n",
        "    print(template.format(epoch + 1,\n",
        "                          train_loss.result(),\n",
        "                          train_accuracy.result() * 100,\n",
        "                          test_loss.result(),\n",
        "                          test_accuracy.result() * 100))\n",
        "    \n",
        "    if epoch != 0 and epoch % 5 == 0 and min(best_accuracies) < (test_accuracy.result() * 100):\n",
        "        index = best_accuracies.index(min(best_accuracies))\n",
        "        best_accuracies[best_accuracies.index(min(best_accuracies))] = test_accuracy.result() * 100\n",
        "        if best_accuracies_epoch[index] != 0:\n",
        "            tf.io.gfile.remove('../models/model_{}/k_{}/model_{}'.format(model_name, k_count, best_accuracies_epoch[index]))\n",
        "        best_accuracies_epoch[index] = epoch \n",
        "        model.save_weights('../models/model_{}/k_{}/model_{}'.format(model_name, k_count, epoch), save_format='h5')\n",
        "\n",
        "    # Reset the metrics for the next epoch\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "  \n",
        "    # Evaluate saved models from previous fold.\n",
        "    # First create folder to save outputs.\n",
        "    fold_evaluation_path = '../models/model_{}_evaluation/k_{}'.format(model_name, k_count)\n",
        "    models_path = '../models/model_{}/k_{}'.format(model_name, k_count)\n",
        "    tf.io.gfile.makedirs(fold_evaluation_path)\n",
        "\n",
        "    # Generate metric sheet for fold models\n",
        "    out_columns = ['Probabilities', 'Prediction', 'Label', 'OrigLabel', 'Name', 'Username', 'Bio', 'Location', 'Url', 'Tweets', 'Following', 'Followers',\n",
        "                 'Likes', 'Media']\n",
        "    metric_columns = ['modelname', 'TP', 'FP', 'FN', 'TN', 'Precision', 'Recall', 'F1Score']\n",
        "    model_outputs = pd.DataFrame(columns=out_columns)\n",
        "    model_metrics = pd.DataFrame(columns=metric_columns)\n",
        "    model_metric_counter = 0\n",
        "\n",
        "    for test_model_name in tf.io.gfile.listdir(models_path):\n",
        "        model.load_weights('{}/{}'.format(models_path, test_model_name))\n",
        "        model_preds, test_labels = [], []\n",
        "        model_output_counter = 0\n",
        "\n",
        "        for text, number, labels in test_ds:\n",
        "            predictions = model(text, number)\n",
        "            prediction = [int(x) for x in tf.math.round(predictions).numpy()][0]\n",
        "            label = [int(x) for x in labels.numpy()][0]\n",
        "            model_preds.append(prediction)\n",
        "            test_labels.append(label)\n",
        "\n",
        "            # print(model_output_counter, list(test_index)[model_output_counter])\n",
        "            row = orig_data.iloc[list(test_index)[model_output_counter], :]\n",
        "            proba = predictions.numpy()[0, 0]\n",
        "            proba = 1 - proba if proba < 0.5 else proba\n",
        "            row_bio = row['bio'] if row['bio'] else ''\n",
        "            row_location = row['location'] if row['location'] else ''\n",
        "            row_url = row['url'] if row['url'] else ''\n",
        "            row_tweets = row['tweets'] if row['tweets'] else ''\n",
        "            row_following = row['following'] if row['following'] else ''\n",
        "            row_followers = row['followers'] if row['followers'] else ''\n",
        "            row_likes = row['likes'] if row['likes'] else ''\n",
        "            row_media = row['media'] if row['media'] else ''\n",
        "            model_outputs.loc[model_output_counter] = [proba, prediction, label, row['label'], row['name'], row['username'], \n",
        "                                                     row_bio, row_location, row_url, row_tweets, row_following, row_followers, \n",
        "                                                     row_likes, row_media]\n",
        "        model_output_counter += 1\n",
        "\n",
        "    confusion = tf.math.confusion_matrix(labels=test_labels, predictions=model_preds, num_classes=2)\n",
        "    confusion_numpy = confusion.numpy()\n",
        "    pres_recall = precision_recall_fscore_support(test_labels, model_preds,  average='weighted')\n",
        "\n",
        "    tp, fp, fn, tn = confusion_numpy[0,0], confusion_numpy[0,1], confusion_numpy[1,0], confusion_numpy[1,1]\n",
        "    precision, recall, f1_score = pres_recall[0], pres_recall[1], pres_recall[2]\n",
        "\n",
        "    current_model = 'model_{}_k_{}_{}'.format(model_name, k_count, test_model_name)\n",
        "    model_metrics.loc[model_metric_counter] = [current_model, tp, fp, fn, tn, precision, recall, f1_score]\n",
        "    model_metric_counter += 1\n",
        "    print('model_{}_k_{}_{} done ...'.format(model_name, k_count, test_model_name))\n",
        "\n",
        "    model_outputs.to_csv('{}_fold{}_{}_preds.csv'.format(model_name, k_count, test_model_name), encoding='utf-8')  \n",
        "    tf.io.gfile.copy('{}_fold{}_{}_preds.csv'.format(model_name, k_count, test_model_name), '{}/{}_fold{}_{}_preds.csv'.format(fold_evaluation_path, model_name, k_count, test_model_name))\n",
        "\n",
        "    model_metrics.to_csv('{}_fold{}_evalutation.csv'.format(model_name, k_count), encoding='utf-8')\n",
        "    tf.io.gfile.copy('{}_fold{}_evalutation.csv'.format(model_name, k_count), '{}/{}_fold{}_evalutation.csv'.format(fold_evaluation_path, model_name, k_count))\n",
        "\n",
        "    \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}